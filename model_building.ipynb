{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranavirohit/cancer-drug-discovery/blob/main/model_building.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fp21ZTJcTOlt"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPPt7coJSJIv",
        "outputId": "3770cc0d-ff8e-42fd-8512-d0c99d37124f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbZ9OUAnSMLS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gI2gGiPwjcNI"
      },
      "outputs": [],
      "source": [
        "from numpy import vstack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIljqb1HEcnP"
      },
      "source": [
        "## PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHB-uOknJ-qq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fn0w6vpEjRwq"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import ReLU\n",
        "from torch.nn import Sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51wr4pHHAUVY"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B-trmWjEitE"
      },
      "source": [
        "## Sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WhBRjPHjaLX"
      },
      "outputs": [],
      "source": [
        "import sklearn as sk\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agg-oCmLEpEQ"
      },
      "source": [
        "## TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_BIhMZNEaJO"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from keras import layers as L"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQebJ8OrTSK1"
      },
      "source": [
        "# Preparing Input Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMiRzN-x0516",
        "outputId": "2e4539cb-caa0-4e17-af0b-f37637265b43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py:3326: DtypeWarning: Columns (27) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ],
      "source": [
        "chembl_all_1_2 = pd.read_csv('/content/drive/MyDrive/Data/chembl_all_1_2.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEKzlU1kHk4W"
      },
      "source": [
        "## SMILES Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Er9vBBK5Xgs5"
      },
      "outputs": [],
      "source": [
        "# Defining SMILES characters (assigning a number value to each character).\n",
        "SMILES_CHARS = [' ',\n",
        "                '#', '%', '(', ')', '+', '-', '.', '/',\n",
        "                '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "                '=', '@',\n",
        "                'A', 'B', 'C', 'F', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P',\n",
        "                'R', 'S', 'T', 'V', 'X', 'Z',\n",
        "                '[', '\\\\', ']',\n",
        "                'a', 'b', 'c', 'e', 'g', 'i', 'l', 'n', 'o', 'p', 'r', 's',\n",
        "                't', 'u']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWXWxW2_OLCP"
      },
      "source": [
        "\n",
        "\n",
        "*   An encoder takes a readable entity (such as the SMILES code) and transforms it into code, such as an array\n",
        "*   A decoder completes the inverse, transforms code into a readable entity\n",
        "> Both are defined as a function to be applied later\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2tu0lDMC_fW"
      },
      "outputs": [],
      "source": [
        "# Defining an encoder and decoder.\n",
        "smi2index = dict( (c,i) for i,c in enumerate( SMILES_CHARS ) )\n",
        "index2smi = dict( (i,c) for i,c in enumerate( SMILES_CHARS ) )\n",
        "\n",
        "def smiles_encoder(smiles, maxlen=120):\n",
        "    X = np.zeros( ( maxlen, len( SMILES_CHARS ) ) )\n",
        "    for i, c in enumerate( smiles ):\n",
        "        X[i, smi2index[c] ] = 1\n",
        "    return X\n",
        "\n",
        "def smiles_decoder( X ):\n",
        "    smi = ''\n",
        "    X = X.argmax( axis=-1 )\n",
        "    for i in X:\n",
        "        smi += index2smi[ i ]\n",
        "    return smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StVy_A6_XoMd"
      },
      "outputs": [],
      "source": [
        "# Subset dataframe just for now so things compute faster.\n",
        "cas1 = chembl_all_1_2[chembl_all_1_2.cancer_status == False].sample(10000)\n",
        "cas2 = chembl_all_1_2[chembl_all_1_2.cancer_status == True]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "206ZXdHVDhkr"
      },
      "outputs": [],
      "source": [
        "chembl_all_1_2_smiles = pd.concat([cas1, cas2]) # Joining both datasets to create dataset used for machine learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7Okwrx9ETtv"
      },
      "outputs": [],
      "source": [
        "chembl_all_1_2_smiles.Smiles = chembl_all_1_2_smiles.Smiles.replace('nan', np.nan) # Replacing NaN values.\n",
        "chembl_all_1_2_smiles = chembl_all_1_2_smiles[chembl_all_1_2_smiles['Smiles'].notna()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZzmtFvMEXoN",
        "outputId": "1ad1e6cb-e166-40ea-8575-43cafed3a428"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10987, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "chembl_all_1_2_smiles = chembl_all_1_2_smiles[['cancer_status', 'Smiles']]\n",
        "chembl_all_1_2_smiles.shape # Shaping SMILES dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZop-i45FUNh"
      },
      "outputs": [],
      "source": [
        "chembl_all_1_2_smiles.to_csv('/content/drive/MyDrive/Data/chembl_all_1_2_smiles.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCuUbpOhTfPB",
        "outputId": "e6e47d67-cf34-48e4-d37d-89621e551572"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "print(smiles_encoder(\"CN\", maxlen =  2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fF0sYC8TghC"
      },
      "outputs": [],
      "source": [
        "chembl_all_1_2_smiles = pd.read_csv('/content/drive/MyDrive/Data/chembl_all_1_2_smiles.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBYVkrLuThXN"
      },
      "outputs": [],
      "source": [
        "max_length = max(chembl_all_1_2_smiles.Smiles.apply(lambda x: len(str(x))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ME7n4A2_BUPr"
      },
      "source": [
        "\n",
        "*   Changing SMILES code into a numpy array, then transforms itinto a flat list instead of a two dimensional array\n",
        "> Flat lists are easier to input into neural network, although it is possible to input the 2-dimensional array instead of a list directly\n",
        "\n",
        "*   Flat lists do not contain any nested lists, such as below (no lists within a list)\n",
        "\n",
        "```\n",
        "my_list = [1, 2, 3, [4, 5], 6]\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kq-3qt-qTiZb"
      },
      "outputs": [],
      "source": [
        "# Creating input dataset for neural network.\n",
        "chembl_all_1_2_smiles_flat = chembl_all_1_2_smiles['Smiles'].apply(lambda x: smiles_encoder(x, maxlen = max_length).flatten())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCtLuCWtAAzO"
      },
      "source": [
        "## Tensor Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFOwPKXcDFGR"
      },
      "outputs": [],
      "source": [
        "# Split test and train dataset here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ahty2mOBAZ8-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d5755ca-71e4-405d-baa7-20c2b5b5c766"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-04148b3231eb>:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
            "  tensor_x = torch.Tensor(chembl_all_1_2_smiles_flat.tolist())\n"
          ]
        }
      ],
      "source": [
        "tensor_x = torch.Tensor(chembl_all_1_2_smiles_flat.tolist())\n",
        "tensor_y = torch.Tensor(chembl_all_1_2_smiles['cancer_status']) # my_y is a list of cancer_status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suiflkbvC7Lk"
      },
      "outputs": [],
      "source": [
        "tensor_y = tensor_y.type(torch.int32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNWd0b6N_X9E"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "x = tf.convert_to_tensor(tensor_x)\n",
        "y = tf.convert_to_tensor(tensor_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8E0-EjXCMvz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04f2ffe1-f1ee-4fbc-d555-8e53f1b07a4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]], shape=(10987, 82768), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Y9j3XQwCQ17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ed90d3c-4d75-4bc6-9b0b-eb18a851f103"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([0 0 0 ... 1 1 1], shape=(10987,), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p38ocBkq9-Tv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33509c1d-4164-4598-d7d1-2cd85b9d206a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10987, 82768)\n"
          ]
        }
      ],
      "source": [
        "# tensor_x_shape = tensor_x.shape\n",
        "# print(tensor_x_shape)\n",
        "# Because PyTorch tensors are now converted into TensorFlow tensors, necessary to shape of those tensors instead\n",
        "tx_shape = x.shape\n",
        "print(tx_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bMvnR3A-GNe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24faf14b-eeec-4c7b-cd91-933b07a3091d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10987,)\n"
          ]
        }
      ],
      "source": [
        "# tensor_y_shape = tensor_y.shape\n",
        "# print(tensor_y_shape)\n",
        "# Because PyTorch tensors are now converted into TensorFlow tensors, necessary to shape of those tensors instead\n",
        "ty_shape = y.shape\n",
        "print(ty_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPWmyiLAx1Xa"
      },
      "outputs": [],
      "source": [
        "# my_dataset = TensorDataset(tensor_x, tensor_y) # create your datset\n",
        "# my_dataloader = DataLoader(my_dataset) # create your dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6B4cHxUPuuR"
      },
      "source": [
        "### Training and Test Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gbnv1d_TPuJp"
      },
      "outputs": [],
      "source": [
        "# X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1Clkwb5Hq8E"
      },
      "source": [
        "# Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu-0_y4GIsHE"
      },
      "source": [
        "## TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCbRcF-HKkzi"
      },
      "outputs": [],
      "source": [
        "model = keras.models.Sequential()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoWhDanm5k8V"
      },
      "outputs": [],
      "source": [
        "# model.add(L.Input((82768,)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wyrc2KpaNmyp"
      },
      "outputs": [],
      "source": [
        "model.add(L.Dense(256, activation = \"relu\"))\n",
        "model.add(L.Dense(128, activation = \"relu\"))\n",
        "model.add(L.Dense(36, activation = \"relu\"))\n",
        "model.add(L.Dense(16, activation = \"relu\"))\n",
        "model.add(L.Dense(1, activation = \"sigmoid\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtmNxXmZKnAu"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 832
        },
        "id": "JS2hg7KXKpuj",
        "outputId": "02dca895-e512-4c08-eb7a-2b16c524725d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "1099/1099 [==============================] - 40s 35ms/step - loss: 0.3169\n",
            "Epoch 2/1000\n",
            "1099/1099 [==============================] - 39s 36ms/step - loss: 0.2461\n",
            "Epoch 3/1000\n",
            "1099/1099 [==============================] - 39s 36ms/step - loss: 0.1484\n",
            "Epoch 4/1000\n",
            "1099/1099 [==============================] - 40s 36ms/step - loss: 0.0499\n",
            "Epoch 5/1000\n",
            "1099/1099 [==============================] - 40s 36ms/step - loss: 0.0176\n",
            "Epoch 6/1000\n",
            "1099/1099 [==============================] - 40s 37ms/step - loss: 0.0088\n",
            "Epoch 7/1000\n",
            "1099/1099 [==============================] - 39s 35ms/step - loss: 0.0085\n",
            "Epoch 8/1000\n",
            "1099/1099 [==============================] - 38s 34ms/step - loss: 0.0045\n",
            "Epoch 9/1000\n",
            "1099/1099 [==============================] - 38s 35ms/step - loss: 0.0040\n",
            "Epoch 10/1000\n",
            "1099/1099 [==============================] - 40s 36ms/step - loss: 0.0022\n",
            "Epoch 11/1000\n",
            "1099/1099 [==============================] - 37s 34ms/step - loss: 0.0039\n",
            "Epoch 12/1000\n",
            "1099/1099 [==============================] - 37s 34ms/step - loss: 0.0057\n",
            "Epoch 13/1000\n",
            "1099/1099 [==============================] - 38s 34ms/step - loss: 0.0068\n",
            "Epoch 14/1000\n",
            "1099/1099 [==============================] - 37s 34ms/step - loss: 0.0077\n",
            "Epoch 15/1000\n",
            " 179/1099 [===>..........................] - ETA: 30s - loss: 0.0028"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-21791f9c832a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1412\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1414\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1415\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1416\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \"\"\"\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    295\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m       raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    316\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1090\u001b[0m     \u001b[0;34m\"\"\"Updates the progbar.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1092\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_init_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1093\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# One-indexed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_maybe_init_progbar\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1067\u001b[0m       \u001b[0;31m# step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m       self.stateful_metrics = self.stateful_metrics.union(\n\u001b[0;32m-> 1069\u001b[0;31m           set(m.name for m in self.model.metrics))\n\u001b[0m\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model.fit(x,y, epochs = 1000, batch_size = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBaixtEfPWJt"
      },
      "outputs": [],
      "source": [
        "accuracy = model.evaluate(x, y)\n",
        "print(\"Model accuracy: %.2f\" %(accuracy * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InUE6abkPi-H"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(x)\n",
        "print([round(x[0]) for x in predictions])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTxsym45Iwko"
      },
      "source": [
        "## PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxibmDvJYa5f"
      },
      "outputs": [],
      "source": [
        "# Defining the conventional neural network.\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_sizes, dropout = 0.5):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        # Defining five layers.\n",
        "        self.conv1 = nn.Conv1d(input_size, hidden_sizes[0], kernel_size = 3, padding = 1)\n",
        "        self.conv2 = nn.Conv1d(hidden_sizes[0], hidden_sizes[1], kernel_size = 3, padding = 1)\n",
        "        self.conv3 = nn.Conv1d(hidden_sizes[1], hidden_sizes[2], kernel_size = 3, padding = 1)\n",
        "        self.conv4 = nn.Conv1d(hidden_sizes[2], hidden_sizes[3], kernel_size = 3, padding = 1)\n",
        "        self.conv5 = nn.Conv1d(hidden_sizes[3], hidden_sizes[4], kernel_size = 3, padding = 1)\n",
        "\n",
        "        # Defining the activation function.\n",
        "        self.relu = nn.ReLU() # Using ReLU, which clearly activates the neuron (negative values do not, positive values do).\n",
        "\n",
        "        # Defining the dropout layer, ensures model is less prone to overfitting.\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Defining the output layer\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_sizes[-1], output_size),\n",
        "            nn.Sigmoid() # Values between 0 and 1?\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Applying the activation function to each layer.\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.relu(self.conv3(x))\n",
        "        x = self.relu(self.conv4(x))\n",
        "        x = self.relu(self.conv5(x))\n",
        "\n",
        "        # Applying the dropout layer and making it a one-dimensional tensor.\n",
        "        x = self.dropout(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Applying the output layer, there is currently only one return value.\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-MLfkBXppax"
      },
      "source": [
        "\n",
        "\n",
        "*   Describe model (# of input layers, etc.)\n",
        "*   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naFuxJELopkY"
      },
      "outputs": [],
      "source": [
        "model = CNN(input_size = 1, hidden_sizes = [10, 10, 10, 10, 10], output_size = 1, dropout = 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLtg9n4-ytPM"
      },
      "outputs": [],
      "source": [
        "# Split the dataset to create a test set (will only use when model is final)\n",
        "train_all, test_final = torch.utils.data.random_split(my_dataloader, [0.8, 0.2], generator=torch.Generator().manual_seed(0))\n",
        "\n",
        "# Sometimes, you might want to know how your model performs at the moment, so you can split again\n",
        "# Splitting the datasets to assess how model is performing at that exact moment\n",
        "train, test = torch.utils.data.random_split(train_all, [0.8, 0.2], generator=torch.Generator().manual_seed(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mHrH3mg3NvY"
      },
      "outputs": [],
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(train, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5wisdcUy4if"
      },
      "outputs": [],
      "source": [
        "len(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMPYu6hjzIjY"
      },
      "outputs": [],
      "source": [
        "# Define the optimizer\n",
        "criterion =  torch.nn.BCELoss() # MSELoss: mean squared loss for regression, BCELoss: Binary cross-entropy loss for binary classification, CrossEntropyLoss: Categorical cross-entropy loss for multi-class classification\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.9)\n",
        "\n",
        "# Loop through the training epochs and make mini-batches for the stochastic gradient descent\n",
        "for epoch in range(10): # usually, > 100, but likely around 1000x\n",
        "  print(epoch)\n",
        "  # enumerate mini batches\n",
        "  for i, (inputs, targets) in enumerate(train_dataloader):\n",
        "    #print(i, inputs, targets)\n",
        "    # clear the gradients\n",
        "    optimizer.zero_grad()\n",
        "    # compute the model output\n",
        "    yhat = model(inputs)\n",
        "    # calculate loss\n",
        "    loss = criterion(yhat, targets)\n",
        "    # credit assignment\n",
        "    loss.backward()\n",
        "    # update model weights\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Urn9zMTt0gkJ"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# PyTorch models inherit from torch.nn.Module\n",
        "class GarmentClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GarmentClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model = GarmentClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDt19_qs0kYr"
      },
      "outputs": [],
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# NB: Loss functions expect data in batches, so we're creating batches of 4\n",
        "# Represents the model's confidence in each of the 10 classes for a given input\n",
        "dummy_outputs = torch.rand(4, 10)\n",
        "# Represents the correct class among the 10 being tested\n",
        "dummy_labels = torch.tensor([1, 5, 3, 7])\n",
        "\n",
        "print(dummy_outputs)\n",
        "print(dummy_labels)\n",
        "\n",
        "loss = loss_fn(dummy_outputs, dummy_labels)\n",
        "print('Total loss for this batch: {}'.format(loss.item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyJG2E9m0npP"
      },
      "outputs": [],
      "source": [
        "# Optimizers specified in the torch.optim package\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTt-GV830qR_"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(epoch_index):\n",
        "    running_loss = 0.\n",
        "    last_loss = 0.\n",
        "\n",
        "    # Here, we use enumerate(training_loader) instead of\n",
        "    # iter(training_loader) so that we can track the batch\n",
        "    # index and do some intra-epoch reporting\n",
        "    for i, data in enumerate(train):\n",
        "        # Every data instance is an input + label pair\n",
        "        inputs, labels = data\n",
        "\n",
        "        # Zero your gradients for every batch!\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Make predictions for this batch\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute the loss and its gradients\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Adjust learning weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Gather data and report\n",
        "        running_loss += loss.item()\n",
        "        if i % 1000 == 999:\n",
        "            last_loss = running_loss / 1000 # loss per batch\n",
        "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
        "            tb_x = epoch_index * len(training_loader) + i + 1\n",
        "            running_loss = 0.\n",
        "\n",
        "    return last_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9n9GPxj0szO"
      },
      "outputs": [],
      "source": [
        "from numpy.core.arrayprint import printoptions\n",
        "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
        "epoch_number = 0\n",
        "\n",
        "EPOCHS = 5\n",
        "\n",
        "best_vloss = 1_000_000.\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print('EPOCH {}:'.format(epoch_number + 1))\n",
        "\n",
        "    # Make sure gradient tracking is on, and do a pass over the data\n",
        "    model.train(True)\n",
        "    avg_loss = train_one_epoch(epoch_number)\n",
        "\n",
        "    # We don't need gradients on to do reporting\n",
        "    model.train(False)\n",
        "\n",
        "    running_vloss = 0.0\n",
        "    for i, vdata in enumerate(validation_loader):\n",
        "        vinputs, vlabels = vdata\n",
        "        voutputs = model(vinputs)\n",
        "        vloss = loss_fn(voutputs, vlabels)\n",
        "        running_vloss += vloss\n",
        "\n",
        "    avg_vloss = running_vloss / (i + 1)\n",
        "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
        "\n",
        "    # Log the running loss averaged per batch\n",
        "    # for both training and validation\n",
        "    print('Training', avg_loss, 'Validation', avg_vloss, epoch_number + 1)\n",
        "\n",
        "    # Track best performance, and save the model's state\n",
        "    if avg_vloss < best_vloss:\n",
        "        best_vloss = avg_vloss\n",
        "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    epoch_number += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSUpjutByiE2"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "\n",
        "my_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnC5m2-3jLmo"
      },
      "outputs": [],
      "source": [
        "# model = MLP(17)\n",
        "\n",
        "# # Define the optimizer\n",
        "# criterion =  torch.nn.BCELoss() # MSELoss: mean squared loss for regression, BCELoss: Binary cross-entropy loss for binary classification, CrossEntropyLoss: Categorical cross-entropy loss for multi-class classification\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.9)\n",
        "\n",
        "# # Loop through the training epochs and make mini-batches for the stochastic gradient descent\n",
        "# for epoch in range(10): # usually, > 100, but likely around 1000x\n",
        "#   print(i)\n",
        "#   # enumerate mini batches\n",
        "#   for i, (inputs, targets) in enumerate(train_dl):\n",
        "#     #print(i, inputs, targets)\n",
        "#     # clear the gradients\n",
        "#     optimizer.zero_grad()\n",
        "#     # compute the model output\n",
        "#     yhat = model(inputs)\n",
        "#     # calculate loss\n",
        "#     loss = criterion(yhat, targets)\n",
        "#     # credit assignment\n",
        "#     loss.backward()\n",
        "#     # update model weights\n",
        "#     optimizer.step()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "fp21ZTJcTOlt"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}